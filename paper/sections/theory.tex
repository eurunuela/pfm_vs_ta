% !TEX root = ../main.tex

\section{Theory}

% \begin{itemize}
%     \item What is deconvolution and different formulations presented as a review.
%     \item Analysis vs synthesis
%     \begin{itemize}
%         \item TA paper but without the spatial regularization
%         \item PFM paper
%         \item In Gitelman it's an \(\mathbf{H}\) multiplied by a Fourier term.
%     \end{itemize}
%     \item Spikes and block models
% \end{itemize}

Conventional general linear model (GLM) analysis puts forward a number of regressors incorporating knowledge about the paradigm or behavior. For instance, the timing of epochs for a certain condition can be modeled as an indicator function $p(t)$, convolved with the HRF $h(t)$, and sampled at TR resolution (\citealt{boynton1996linear}):\todo{Rather some Friston paper(s) here}
$$
   p(t) \rightarrow p*h(t) \rightarrow x[k] = p*h(k\cdot\text{TR}).
$$
The vector $\mathbf{x}=[x[k]]_{k=1,\ldots,N}$ then constitutes the hypothetical response, and several of them can be stacked as the columns of the design matrix $\mathbf{X}=[\mathbf{x}_1 \ldots \mathbf{x}_L]$, leading to the celebrated GLM: 
\begin{equation}
    \label{eq:glm}
    \mathbf{y} = \mathbf{X \beta} + \mathbf{e},
\end{equation}
where the empirical timecourse $\mathbf{y}$ is explained by a linear combination of the regressors in $\mathbf{X}$ weighted by the parameter weights in $\mathbf{\beta}$ and corrupted by additive noise $\mathbf{e}$. Under independent and identically distributed Gaussian assumptions of the latter, the maximum likehood estimate of the parameter weights reverts to the ordinary least-squares estimator; i.e., minimizing the residual sum of squares between the fitted model and measurements. The number of regressors $L$ is typically much less than the number of measurements $N$, and thus the regression problem is over-determined and does no require additional constraints or assumptions.

In the deconvolution approach, no prior knowledge is taken into account, and the purpose is to estimate the deconvolved activity-inducing signal $\mathbf{s}$ from the measurements that are ideally fitted as $y(t) = s*h(t)$. In the discrete domain, this can be formulated as the signal model
\begin{equation}
    \label{eq:deconvolution}
    \mathbf{y} = \mathbf{Hs} + \mathbf{e},
\end{equation}
where $\mathbf{H}$ is an $N\times N$ Toeplitz matrix that represents the discrete convolution with the HRF, and $\mathbf{s}$ a length-$N$ vector with the unknown activity-inducing signal. Despite the apparent similarity with the GLM equation, there are two important differences. First, the multiplication with the design matrix is an expansion as a weighted linear combination of its columns, while the multiplication with the HRF matrix represents a convolution with its shifted rows. Second, determining $\mathbf{s}$ is an ill-posed problem given the nature of the HRF; e.g., as can be seen intuitively, the rows of $\mathbf{H}$ are highly correlated due to large overlap between shifted HRFs (see Figure~\ref{fig:hrf_diff}B), thus introducing ambiguity and instability in the estimates of $\mathbf{s}$. Therefore, additional assumptions under the form of regularization are needed. For the purpose of this paper, it will be useful to consider also the model
$$
\mathbf{y} = \mathbf{H L u} + \mathbf{e},
$$
where the activity-inducing signal $\mathbf{s}$ is rewritten in terms of the innovation signal $\mathbf{u}$; i.e., the derivative $\mathbf{Ds}=\mathbf{u}$ of $\mathbf{s}$, or, equivalently, $\mathbf{s}=\mathbf{Lu}+c$, where $\mathbf{L}$ is the integrator and $c$ a constant (\citealt{cherkaoui2019SparsitybasedBlindDeconvolution,urunuela2020StabilityBasedSparseParadigm}). 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Paradigm Free Mapping
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Synthesis-based deconvolution}

Synthesis-based deconvolution models are those in which the candidate estimator is synthesized (i.e. constructed) from a linear combination of dictionary atoms. Paradigm Free Mapping (PFM) is based on such construction as its estimator \(\mathbf{H}\) is described by shifted HRFs (atoms). Hence, considering the signal model introduced in~\eqref{eq:gitelman}; i.e., the BOLD signal (\(\mathbf{x}\)) is the result of convolving the underlying neural activity (\(\mathbf{s}\)) with the hemodynamic response (\(\mathbf{H}\)), the activity-inducing signal can be estimated by solving the following regularized least-squares problem (\citealt{gaudes2011DetectionCharacterizationSingletrial,caballerogaudes2013ParadigmFreeMapping,urunuela2020StabilityBasedSparseParadigm}):
\begin{equation}
    \label{eq:pfm}
    \hat{\mathbf{s}} = \arg \min_{\mathbf{s}} \frac{1}{2} \| \mathbf{y} - \mathbf{Hs} \|_2^2 + \Omega(\mathbf{s}),
\end{equation}
where \(\Omega(\mathbf{s})\) is the regularization term.

Assuming that single-trial BOLD responses are the result of brief bursts of neuronal activation, the activity-inducing signal \(\mathbf{s}\) must be a sparse vector. Thus, sparse estimates of \(\mathbf{s}\) could be obtained by substituting \(\Omega(\mathbf{s})\) in~\eqref{eq:pfm} with an \(l_0\)-norm and solving the optimization problem (\citealt{bruckstein2009SparseSolutionsSystems}). However, due to the convolution model defined in~\eqref{eq:pfm}, finding the optimal solution to the problem demands an exhaustive search across all possible combinations of the columns of the design matrix \(\mathbf{H}\). Hence, a pragmatic solution is to solve the optimization problem with the use of an \(l_1\)-norm, or LASSO (\citealt{tibshirani1996RegressionShrinkageSelection}), which is a convex function and therefore provides fast convergence to the optimal solution.
\begin{equation}
    \label{eq:pfm_spike}
    \hat{\mathbf{s}} = \arg \min_{\mathbf{s}} \frac{1}{2} \| \mathbf{y} - \mathbf{Hs} \|_2^2 + \lambda \| \mathbf{s} \|_1,
\end{equation}
where \(\lambda\) regulates how sparse the optimal solution is.

Such formulation provides flexibility to expand the capabilities of PFM. For instance, incorporating the integration operator \(\mathbf{L}\) into the design matrix \(\mathbf{H}\) allows the recovery of the innovation signal \(\mathbf{u}\); i.e., the derivative of the activity-inducing signal \(\mathbf{s}\). Therefore, the innovation signal can be estimated by solving the following optimization problem (\citealt{cherkaoui2019SparsitybasedBlindDeconvolution,urunuela2020StabilityBasedSparseParadigm}):
\begin{equation}
    \label{eq:pfm_block}
    \hat{\mathbf{u}} = \arg \min_{\mathbf{u}} \frac{1}{2} \| \mathbf{y} - \mathbf{HLu} \|_2^2 + \lambda \| \mathbf{u} \|_1.
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Total Activation
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Analysis-based deconvolution}

On the other hand, the estimator of the signal is analyzed in analysis-based deconvolution models, i.e., some aspects of it are calculated and penalized during the estimation process. For instance, Total Activation (TA) proposes to use a linear differential operator \(L_h\) that inverts the hemodynamic system based on activelets to recover the activity-inducing signal \(\mathbf{s}\) (\citealt{karahanoglu2013TotalActivationFMRI,khalidov2011activelets,karahanoglu2011SignalProcessingApproacha}):
\begin{equation}
    L_h\{x\}(t) = s(t)
\end{equation}
where \(x\) is the neuronal-related signal; i.e., the activity inducing signal \(\mathbf{s}\) convolved with the HRF, and \(L_h\) is defined as
\begin{equation}
    L_h\ = \prod_{i=1}^{M_1} (D-\alpha_i I) (\prod_{j=1}^{M_2} (D - \gamma_j I))^{-1},
\end{equation}
where \(D\) is the derivative operator, \(\alpha_i (i=1, \hdots, M_1)\) define the zeros of the filter, \(\gamma_j (j=1, \hdots, M_2)\) represent the poles, \(I\) is the identity matrix and \(M_1 > M_2\). Given the relationship between the activity-inducing and the innovation signal, the latter can be recovered as:
\begin{equation}
    L\{x\}(t) = D\{s\}(t) = u(t)
\end{equation}
where \(L = DL_h\) and \(D\) is the derivative.

Therefore, for a given voxel, the neuronal-related signal could be estimated by solving the following regularized least-squares problem:
\begin{equation}
    \hat{\mathbf{x}} = \arg \min_{\mathbf{x}} \frac{1}{2} \| \mathbf{y} - \mathbf{x} \|_2^2 + \Omega(\mathbf{x}),
\end{equation}
where \(\mathbf{y}\) is the fMRI data and \(\mathcal{R}(\mathbf{x})\) is the following \(l_1\)-norm regularization term:
\begin{equation}
    \hat{\mathbf{x}} = \arg \min_{\mathbf{x}} \frac{1}{2} \| \mathbf{y} - \mathbf{x} \|_2^2 + \lambda \| \Delta_L \{\mathbf{x}\} \|_1,
\end{equation}
where \(\lambda\) is the regularization parameter.

This work evaluates the core of the two techniques, i.e., the regularized least-squares problem with temporal regularization, which corresponds to the generalized total-variation operator in Total Activation. Therefore, we do not study the impact of spatial constraints, as we assume that spatial regularization terms should perform identically on both methods.

\subsection{Selection of the regularization parameter}
\label{sec:regparam}

The correct selection of the regularization parameter \(\lambda\) is a critical decision for the accurate performance of deconvolution methods. Even though many techniques have been proposed in the literature, the optimal strategy that selects \(\lambda\) is yet to be found. Algorithms like least angle regression (LARS) (\citealt{efron2004LeastAngleRegression}) provide all the possible solutions to the optimization problem and their corresponding value of \(\lambda\), i.e., the regularization path, but don't provide the optimal solution. Therefore, strategies that exploit the regularization path can provide a selection of \(\lambda\) that is close to the optimal. For instance, in Paradigm Free Mapping, the optimal result is given by the Bayesian Information Criterion (BIC) (\citealt{schwarz1978EstimatingDimensionModel}), i.e., the regularization path solution with the minimum BIC is considered optimal. Another approach could be to update the regularization parameter \(\lambda\) on every iteration \(n\) like Total Activation does, so that the residuals converge to a previously estimated noise level of the data fit \(\tilde{\sigma}\). The pre-estimated noise level is calculated from the median absolute deviation of fine-scale wavelet coefficients (Daubechies, order 3) (\citealt{karahanoglu2013TotalActivationFMRI}):
\begin{equation}
    \lambda^{n+1} = \frac{N \tilde{\sigma}}{\frac{1}{2} \| \mathbf{y} - \mathbf{x}^n \|_F^2} \lambda^n.
\label{eq:std}
\end{equation}

Here, we compare the performance of the two deconvolution algorithms with both selection criteria and in terms of the estimation of the activity-inducing signal \(\mathbf{s}\) using the \textit{spike model} in~\eqref{eq:pfm_spike} and the innovation signal \(\mathbf{u}\) using the \textit{block model} in~\eqref{eq:pfm_block}.